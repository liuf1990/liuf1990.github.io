<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-60320583-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-60320583-2');
</script>

<title>Feng Liu - Home</title>
<link rel="stylesheet" type="text/css" href="style.css">

<script type="text/javascript" src="js/hidebib.js"></script>

</head>
<body>

<div class="section">
<h1>Feng Liu</h1>
</div>
<hr>

<div class="section">
<table>
  <tr valign="top"> <td style="width: 600px; vertical-align: top;">
  I am currently a postdoctoral researcher in the <a href = "http://www.cvlab.cse.msu.edu/">Computer Vision Lab</a> at Michigan State University. I am fortunate to be advised by <a href = "http://www.cse.msu.edu/~liuxm/index2.html">Xiaoming Liu</a>. I previously graduated with Ph. D. degree in Computer Science from Sichuan University where I was advised by <a href = "http://rsc.scu.edu.cn/info/1147/1883.htm">Zhisheng You</a> and <a href = "http://scubrl.org/qjzhao">Qijun Zhao</a>. 

  <p><br></p>
  My research interests span the areas of joint analysis of 2D images and 3D shapes, including 3D modeling, semantic correspondence, and coherent 3D scene reconstruction.
  <p><br></p>
  <p>
    <a href="javascript:toggleblock('cv')">cv</a> | <a href="javascript:toggleblock('email')">email</a> | <a href="https://github.com/liuf1990">github</a>  | <a href="https://scholar.google.com/citations?user=oZT8t6kAAAAJ">google scholar</a>
  </p>
  <pre xml:space="preserve" id="email" style="font-size: 12px">

liufeng6@msu.edu
  </pre>
  <script xml:space="preserve" language="JavaScript">
  hideblock('email');
  </script>
  </td>

  <td width="400"><img src="img.jpg" alt="My picture" height=200 align="right"/></td>
    </tr>
  </table>
</div>


<div class="section">
<h2> News </h2>
</br>
 &nbsp &nbsp &nbsp <strong>[Sep&nbsp 2020]</strong>    &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  &nbsp &nbsp &nbsp A paper is accepted by NeurIPS 2020 as oral presentation (1.1% acceptance rate)
<!--------------------------------------------------------------------------->
</br>
 &nbsp &nbsp &nbsp <strong>[May 2020]</strong>    &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  &nbsp &nbsp A paper is accepted by TPAMI
<!--------------------------------------------------------------------------->
</br>
 &nbsp &nbsp &nbsp <strong>[Feb&nbsp 2020]</strong>    &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  &nbsp &nbsp &nbsp A paper is accepted by CVPR 2020

</div>

<div class="section">
<h2> Publications </h2><br><div class="year_heading"><br>2020<hr width="220px" align="left"></div>
<!--------------------------------------------------------------------------->
<div class="paper" id="neurips20">
<img class="paper" src="figures/nips20_teaser.png" />
<p> <strong style="color:red">[New]</strong> <b id="papertitle">Learning Implicit Functions for Topology-Varying Dense 3D Shape Correspondence</b> <br/> 
<strong>Feng Liu</strong>, Xiaoming Liu <br/> 
NeurIPS, 2020 (<strong>Oral presentation</strong>)<br/> 
<a href="javascript:toggleblock('neurips20Bib')">bibtex </a>  &nbsp
<a href="javascript:toggleblock('neurips20Abs')">abstract </a>  &nbsp
<a href="http://cvlab.cse.msu.edu/project-implicit-dense-correspondence.html">project page </a>  &nbsp
<a href="http://cvlab.cse.msu.edu/pdfs/Implicit_Dense_Correspondence.pdf">pdf </a>  &nbsp    
<a href="https://youtu.be/HsS6bQisdz0">video </a>  &nbsp 
<a href="http://cvlab.cse.msu.edu/posters/NIPS20_feng_poster.pdf">poster </a>  &nbsp 
<a href="https://github.com/liuf1990/Implicit_Dense_Correspondence">code </a> </p>
<div class="papermeta" id="neurips20Meta">
<em id="neurips20Abs">The goal of this paper is to learn dense 3D shape correspondence for topology-varying objects in an unsupervised manner. Conventional implicit functions estimate the occupancy of a 3D point given a shape latent code. Instead, our novel implicit function produces a part embedding vector for each 3D point, which is assumed to be similar to its densely corresponded point in another 3D shape of the same object category. Furthermore, we implement dense correspondence through an inverse function mapping from the part embedding to a corresponded 3D point. Both functions are jointly learned with several effective loss functions to realize our assumption, together with the encoder generating the shape latent code. During inference, if a user selects an arbitrary point on the source shape, our algorithm can automatically generate a confidence score indicating whether there is a correspondence on the target shape, as well as the corresponding semantic point if there is one. Such a mechanism inherently benefits man-made objects with different part constitutions. The effectiveness of our approach is demonstrated through unsupervised 3D semantic correspondence and shape segmentation.</em>
<pre xml:space="preserve" id="neurips20Bib">

@inproceedings{ learning-implicit-functions-for-topology-varying-dense-3d-shape-correspondence,
  author = { Feng Liu and Xiaoming Liu },
  title = { Learning Implicit Functions for Topology-Varying Dense 3D Shape Correspondence },
  booktitle = { In Proceeding of 2020 Conference on Neural Information Processing Systems },
  address = { Virtual },
  month = { December },
  year = { 2020 },
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('neurips20Abs');
hideblock('neurips20Bib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="tpami20">
<img class="paper" src="figures/tpami20_teaser.png" />
<p> <b id="papertitle">Gait Recognition via Disentangled Representation Learning</b> <br/> 
Ziyuan Zhang, Luan Tran, <strong>Feng Liu</strong>, Xiaoming Liu <br/> 
IEEE TPAMI, 2020 <br/> 
<a href="javascript:toggleblock('tpami20Bib')">bibtex </a>  &nbsp
<a href="javascript:toggleblock('tpami20Abs')">abstract </a>  &nbsp
<a href="http://cvlab.cse.msu.edu/project-gaitnet.html">project page </a>  &nbsp
<a href="http://cvlab.cse.msu.edu/pdfs/Zhang_Tran_Liu_Liu_arxiv2019.pdf">pdf </a>  &nbsp    
<a href="http://cvlab.cse.msu.edu/frontal-view-gaitfvg-database.html">dataset </a>  &nbsp 
<a href="https://github.com/ziyuanzhangtony/GaitNet-CVPR2019">code </a> </p>
<div class="papermeta" id="tpami20Meta">
<em id="tpami20Abs">Gait, the walking pattern of individuals, is one of the most important biometrics modalities. Most of the existing gait recognition methods take silhouettes or articulated body models as the gait features. These methods suffer from degraded recognition performance when handling confounding variables, such as clothing, carrying and view angle. To remedy this issue, we propose a novel AutoEncoder framework to explicitly disentangle pose and appearance features from RGB imagery and the LSTM-based integration of pose features over time produces the gait feature. In addition, we collect a Frontal-View Gait (FVG) dataset to focus on gait recognition from frontal-view walking, which is a challenging problem since it contains minimal gait cues compared to other views. FVG also includes other important variations, e.g., walking speed, carrying, and clothing. With extensive experiments on CASIA-B, USF and FVG datasets, our method demonstrates superior performance to the state of the arts quantitatively, the ability of feature disentanglement qualitatively, and promising computational efficiency.</em>
<pre xml:space="preserve" id="tpami20Bib">

@article{ on-learning-disentangled-representations-for-gait-recognition,
  author = { Ziyuan Zhang and Luan Tran and Feng Liu and Xiaoming Liu },
  title = { On Learning Disentangled Representations for Gait Recognition },
  journal = { IEEE Transactions on Pattern Analysis and Machine Intelligence },
  month = { May },
  year = { 2020 },
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('tpami20Abs');
hideblock('tpami20Bib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->
<!--------------------------------------------------------------------------->
<div class="paper" id="cvpr20">
<img class="paper" src="figures/cvpr20_teaser.png" />
<p> <b id="papertitle">On the Detection of Digital Face Manipulation</b> <br/> 
<strong>Feng Liu*</strong>, Hao Dang*, Joel Stehouwer*, Xiaoming Liu, Anil Jain <br/> 
CVPR, 2020 <br/> 
<a href="javascript:toggleblock('cvpr20Bib')">bibtex </a>  &nbsp
<a href="javascript:toggleblock('cvpr20Abs')">abstract </a>  &nbsp
<a href="http://cvlab.cse.msu.edu/project-ffd.html">project page </a>  &nbsp
<a href="http://cvlab.cse.msu.edu/pdfs/dang_liu_stehouwer_liu_jain_cvpr2020.pdf">pdf </a>  &nbsp    
<a href="http://cvlab.cse.msu.edu/posters/CVPR20_fakeface_poster.pdf">poster </a>  &nbsp 
<a href="http://cvlab.cse.msu.edu/dffd-dataset.html">dataset </a>  &nbsp 
<a href="https://github.com/JStehouwer/FFD_CVPR2020">code </a> </p>
<div class="papermeta" id="cvpr20Meta">
<em id="cvpr20Abs">Detecting manipulated facial images and videos is an increasingly important topic in digital media forensics. As advanced face synthesis and manipulation methods become available, new types of fake face representations are being created and raise significant concerns for their implications in social media. Hence, it is crucial to detect the manipulated face image and localize manipulated regions. Instead of simply using multi-task learning to simultaneously detect manipulated images and predict the manipulated mask (regions), we propose to utilize the attention mechanism to process and improve the feature maps of the classification model. The learned attention maps highlight the informative regions to further improve the binary classification, and also visualize the manipulated regions. In addition, to enable our study of manipulated face detection and localization, we collect a large-scale database that contains numerous types of facial forgeries. With this dataset, we perform a thorough analysis of data-driven fake face detection. We demonstrate that the use of an attention mechanism improves manipulated region localization and fake detection.</em>
<pre xml:space="preserve" id="cvpr20Bib">

@inproceedings{ on-the-detection-of-digital-face-manipulation,
  author = { Hao Dang* and Feng Liu* and Joel Stehouwer* and Xiaoming Liu and Anil Jain },
  title = { On the Detection of Digital Face Manipulation },
  booktitle = { In Proceeding of IEEE Computer Vision and Pattern Recognition },
  address = { Seattle, WA },
  month = { June },
  year = { 2020 },
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('cvpr20Abs');
hideblock('cvpr20Bib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<div class="year_heading"><br>2019<hr width="220px" align="left"></div>

<!--------------------------------------------------------------------------->



</div>

<div class="section">
<h2> Academic Services</h2>
</br>
&nbsp &nbsp  <strong>Conference Reviewer:</strong>  &nbsp &nbsp &nbsp &nbsp  &nbsp &nbsp &nbsp 
CVPR {2019, 2020, 2021}, ICCV 2019, ECCV 2020, AAAI {2020, 2021}, IJCAI 2019, </br>
&nbsp &nbsp &nbsp &nbsp  &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  &nbsp &nbsp &nbsp
&nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  
  ACCV 2020, WACV2020, ICB 2019, FG 2019
<!--------------------------------------------------------------------------->
</br> </br>
&nbsp &nbsp <strong>Journal Reviewer:</strong>    &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  &nbsp &nbsp  
TPAMI, TIFS, TIP, PR, TMM, TOMM
<!--------------------------------------------------------------------------->
</div>

</br>
&nbsp &nbsp &nbsp &nbsp   @Website inspired from <a href = "https://shubhtuls.github.io/">here</a>.


</body>
</html>
